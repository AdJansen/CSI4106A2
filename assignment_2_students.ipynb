{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 15976), started 4 days, 0:53:10 ago. (Use '!kill 15976' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b0446a6a9f46b54a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b0446a6a9f46b54a\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as python_random\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "from PIL import Image\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# for reproducibility purposes\n",
    "SEED = 123\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# load tensorboard extension\n",
    "%reload_ext tensorboard\n",
    "# specify the log directory where the tensorboard logs will be written\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the relevant datasets (15/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data shape: \n",
      "(15026, 4)\n",
      "\n",
      "Val Data shape: \n",
      "(3757, 4)\n",
      "\n",
      "Test Data shape: \n",
      "(4696, 4)\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Train Data coloumns: \n",
      "['age' 'ethnicity' 'gender' 'img_name']\n",
      "\n",
      "Val Data coloumns: \n",
      "['age' 'ethnicity' 'gender' 'img_name']\n",
      "\n",
      "Test Data coloumns: \n",
      "['age' 'ethnicity' 'gender' 'img_name']\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Train Data Gender Distribution:\n",
      "0    7864\n",
      "1    7162\n",
      "Name: gender, dtype: int64\n",
      "\n",
      "Val Data Gender Distribution:\n",
      "0    1965\n",
      "1    1792\n",
      "Name: gender, dtype: int64\n",
      "\n",
      "Test Data Gender Distribution:\n",
      "0    2456\n",
      "1    2240\n",
      "Name: gender, dtype: int64\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Train Data Ethnicity Distribution:\n",
      "0    6372\n",
      "1    2869\n",
      "3    2524\n",
      "2    2186\n",
      "4    1075\n",
      "Name: ethnicity, dtype: int64\n",
      "\n",
      "Val Data Ethnicity Distribution:\n",
      "0    1593\n",
      "1     717\n",
      "3     632\n",
      "2     547\n",
      "4     268\n",
      "Name: ethnicity, dtype: int64\n",
      "\n",
      "Test Data Ethnicity Distribution:\n",
      "0    1991\n",
      "1     896\n",
      "3     790\n",
      "2     683\n",
      "4     336\n",
      "Name: ethnicity, dtype: int64\n",
      "\n",
      "_____________________________________________\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2160x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the datasets using the csv files train, val and test \n",
    "# (3)\n",
    "dfTrain = pd.read_csv('./data/train.csv')\n",
    "dfVal = pd.read_csv('./data/val.csv')\n",
    "dfTest = pd.read_csv('./data/test.csv')\n",
    "\n",
    "# print the shapes of the dataframes \n",
    "# (3)\n",
    "print(f\"Train Data shape: \\n{dfTrain.shape}\\n\")\n",
    "print(f\"Val Data shape: \\n{dfVal.shape}\\n\")\n",
    "print(f\"Test Data shape: \\n{dfTest.shape}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "# print the column names from either one of the dataframes \n",
    "# (1)\n",
    "print(f\"Train Data coloumns: \\n{dfTrain.columns.values}\\n\")\n",
    "print(f\"Val Data coloumns: \\n{dfVal.columns.values}\\n\")\n",
    "print(f\"Test Data coloumns: \\n{dfTest.columns.values}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "# print the proportional distribution of gender in all three datasets(i.e., number of male and female) \n",
    "# (3)\n",
    "print(f\"Train Data Gender Distribution:\\n{dfTrain['gender'].value_counts()}\\n\")\n",
    "print(f\"Val Data Gender Distribution:\\n{dfVal['gender'].value_counts()}\\n\")\n",
    "print(f\"Test Data Gender Distribution:\\n{dfTest['gender'].value_counts()}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "# print the proportional distribution of ethnicity in all three datasets \n",
    "# (3)\n",
    "print(f\"Train Data Ethnicity Distribution:\\n{dfTrain['ethnicity'].value_counts()}\\n\")\n",
    "print(f\"Val Data Ethnicity Distribution:\\n{dfVal['ethnicity'].value_counts()}\\n\")\n",
    "print(f\"Test Data Ethnicity Distribution:\\n{dfTest['ethnicity'].value_counts()}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "# plot the age distribution from the training dataset where the x-axis plots the age and the y-axis depicts the count of individuals within each age group. For example, individuals with age=1 are: \n",
    "# (2)\n",
    "\n",
    "graph = dfTrain['age'].value_counts().plot(kind='bar', figsize = (30,12), xlabel = \"Age\", ylabel = \"Age Count\")\n",
    "graph.xaxis.label.set_color('orange')        #setting up X-axis label color to white\n",
    "graph.yaxis.label.set_color('orange')          #setting up Y-axis label color to white\n",
    "graph.tick_params(axis='x', colors='orange')    #setting up X-axis tick color to white\n",
    "graph.tick_params(axis='y', colors='orange')  #setting up Y-axis tick color to white\n",
    "graph.spines['left'].set_color('orange')        # setting up Y-axis tick color to white\n",
    "graph.spines['top'].set_color('orange')         #setting up above X-axis tick color to white"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the ImageDataGenerators (22/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15026 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "Found 3757 validated image filenames.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD6CAYAAABnLjEDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAfl0lEQVR4nO2da4he13WG3yVZtpXIkjzySBrN2JUaCzv50SZBmIT0R3BqcJ2L/SOUXCguGPynBYekJE4LpYEWnD+5QEuKqUNUCHHu2JiU4roOIVAcK7HjxjaOFN80ju4XX3KxJXn3x3xj9L371Zw138ycGXW/DwjN2bPPOfvsc9acb73fWmtHKQXGmP//rFruARhj+sHGbkwj2NiNaQQbuzGNYGM3phFs7MY0woKMPSKuj4inImJfRNy+WIMyxiw+Mer37BGxGsAvAVwHYBrAwwA+Wkp54lz7jI2NlcnJyaG2119/fWj7ggsuqPZbtWr4b9Jrr71W9Tl9+vScxwWAM2fOdPbh4/A2APCcZeewz5gGPldEVH24LdMnux/D9xAAVq9e3bmfukdd41Hn4jbVZ82aNZ1t6vnkuebnTPVZLPjap6encfz4cXlD6pHnuQbAvlLK04OT3g3gRgDnNPbJyUl8//vfH2r77W9/O7Q9Pj5e7ffmN795aPv555+v+hw9enRo+3e/+13V59ixY0Pbp06dqvocOXJkzuMCwO9///uhbXUj1R+JzIPLZAwp84dNPaQXXnjh0LZ62NX5165dO7StDIfnZN26dVWfjRs3Vm3MK6+80tnnoosuGtrm5yXbZ/PmzVXbtm3bhrbHxsaqPjz/L7/8ctWHnxlF5l5zH/6D+cEPfvCc+y7kY/wkgP1nbU8P2owxK5AlF+gi4taI2BMRe44fP77UpzPGnIOFGPsLAC4/a3tq0DZEKeXOUsquUsou9RHIGNMPC/HZHwawMyJ2YMbIPwLgY3PtsGrVKrzpTW8aamPfUokb7H8rX5vblIjHfdQnDfa3lK/Fx1Y+sxqj8uMZ5Tcz7LcpzYDHlBEj2YcHtIjGbcpnZ1599dWqjedRnSsjzvJ+au5ZsxhVMFP78RgzIrOCryMzxoyf/8a40j2JUsrpiPhrAP8JYDWAr5ZSHh/1eMaYpWUhb3aUUn4A4AeLNBZjzBLiCDpjGmFBb/b5EhGVT8rbymdn3zLjsytfm31/9V08+4SZ78uz/jn7V8q36/oeVaHmjH1E5f9xm/Kr1Rh5TJlAE+Wz8vyzngPUz4fy2fl+ZLQRNZ5MAFEmpkDBc5Y5V+aezcdn95vdmEawsRvTCDZ2YxrBxm5MI/Qq0JVSKjGLBQ8lrvA+KkCE237zm99UfTipQglSmXPxGLNZTixkqSAWPr86Nos9SsRT+zGZxJyMQDmq+MciaibrLCOsjdons5+aa07mUnPGbZlgpcXGb3ZjGsHGbkwj2NiNaYTeffYu30X5mpnEE/b/Tp48WfXhJBcVoMG+pvKruS2bCMOMGqDBc6gCKzLVdDJJSBdffHHn+TPXoeaIg2oyiTjKr8341ZkEo0zyVGY/pU9wmxqjCk5i5hNEw/jNbkwj2NiNaQQbuzGNYGM3phF6FeiAWszhqp9K3OAAGRUww20c6ADUwtIll1xS9WFhSYkmPGYlmmSy7pRoxdeh+oxSyloFemSET9WmhCwmk+XFgmAmm1EJbZlgpYwQrLIg+XkcNSuT50zNB9tGpg9f11yBUn6zG9MINnZjGsHGbkwj9F6ppssvyazkovxx9pFV0AKvQHLppZdWffjYyj/lMSs/Up2fA1SUr80rlWQSg0atpsPjVn2U9pDRDDIVV9m3zlTFUUE+fBylxfB+mXlV/UZdxiqz9FiGrnmdS7/xm92YRrCxG9MINnZjGsHGbkwj9J711pVppQJmOIMtk52khJwNGzYMbSuRhqvZsDio9lNBHKosMgfj8DZQC3QqyCiT+ZSpjMJzlhHa1LFUHxaO1HzwPVLzwXObmVe1PDQfRwnB6voz5b8za7gz6tnLZAp2BdE4qMYYY2M3phVs7MY0Qq8+u1qy+cCBA0Pbhw4dqvZT/hXDvi5XpQGAZ555Zmhb+U3so7/44otVH/abWAtQfYCcr83+Xsb/U34aaw9qDnm/bBWUTFAR+818f4Da11Z91q5dO7Q9NjbWeRzVhwNt1L1X+ggfWz1XrFmoZJlRqgYrLYT1Kg56ss9ujLGxG9MKNnZjGsHGbkwj9F6phmGhRAlJL7300pz7ALWQpYQUPo4SQFiQmZqaqvqMj48PbasgjkyACItPQC2sZTL8VGYaowRDFnMya5+rY6lrZSFWZaKxILd+/fqqD++nxEAWDDOBSOreq+AoPr+6Dg4Ey2TvZQJ4FAtZIspvdmMawcZuTCN0GntEfDUiDkfEL85qG4uI+yNi7+D/ugqEMWZFkfHZvwbgnwH8+1lttwN4oJRyR0TcPtj+TNeBzpw5gxMnTgy1HT58eGibg2yAOtBG+S3s/ym/KeMjbtq0aWj7iiuuqPps27ZtaFsF1Si6AiIA4Pnnnx/aVok47NerAI1MZRQO9FDjUf44z63qk0n6Yf9b+eOcLKPuK2s4KpmK9ZpMJaNztTGZRBjuk6kInFl2O1PFd5bON3sp5UcAjlPzjQB2D37eDeCmzlEZY5aVUX32LaWU2VfwQQBbFmk8xpglYsECXZn5PHLOzyQRcWtE7ImIPceP8wcEY0xfjGrshyJiAgAG/x8+V8dSyp2llF2llF0qQcEY0w+jBtXcC+BmAHcM/r8ns9OpU6fw61//eqhtenp6aHv//v3VfizqqcAGFslUoMtll102tL1jx47OPuo4LLaoPuoPGwf6qGtlQUgFunDACotPQC3IZZajymR9AbUopIQ13k8FEPG8ZQQyJaxllqPiOVIinhIxWdRVmXl8/WqMmaWdRlnWaz5kvnr7BoD/AXBVRExHxC2YMfLrImIvgD8dbBtjVjCdb/ZSykfP8av3LfJYjDFLiCPojGmEXhNhTp8+jaNHjw61HTlyZGibK8kCuWV72f/bsqX+NpD9xq1bt1Z92CdTlWo4yGfz5s1VHxVYwdrDE088UfVhH1X5kez/ZSqaqIQa7qOCOJQfz4Eu6vwcsKTuB+sjah5Z+1DXweNWyVT8TZCqiJRZajmDmg9G+eMcHJSpQJutLgT4zW5MM9jYjWkEG7sxjWBjN6YRehXoXnvttSqohoUTJUqwaMaVYoBa3FF9OBiFxUKgFkCU0MZikxKfVNUTPt/evXurPhMTE0PbSiDjrDcVVMNioBIaWfxTwSCqjYVO1YcFU84mVG0qWIqDc5Rgyc+MCgTKrM+ungcW7dTzwCKZEs0yQhpfWybrjcVAr89ujLGxG9MKNnZjGsHGbkwj9CrQnTp1Ci+88MJQG0fQqewsFoSUCMEZZZnIJy6JBdRC3/bt26s+XKpKZbip3H3OclPlrlnsUvPBpaoOHjxY9eHyXupcXecGdMktFh9V5BkLWSpbjEUzJUjxuFmcBOqousy692o8qgRYRiTLCHSZEtDzWWt9lky03ht90z2NMec1NnZjGsHGbkwj9OqznzlzpvLBuGKIympi/48Dc4Da/1aVSPjYKvCF/T11Ls7MU8EgqgrNs88+O7SdyTJTPirrAax7AHWAiJoPDixRASMqqIf7qSwz9lGVj6wq/HSNkYOFgPp+qOOyPqDuvfKr2f/OBB5ljjOfbLW5mM9x/WY3phFs7MY0go3dmEawsRvTCL0KdK+//nolFLGQpAQ6Fh1UsAGLNEqQYmFJiRks3HCpawB4+OGHh7aVQMcll4Ba/FMBETwmJdBxSSW1Ph5nuaksLybTB6ivQ5V44jGprDcW9jLryinhk4N6VNlqDk5SWYBKMOXnQfXhY6sgIw4EU3PN+2UCevi4cwXi+M1uTCPY2I1pBBu7MY3QeylpDorgoA3lp7D/rZJcOFhHBYNkgh+YTGUShfLJ+NqUb8dJLSpghgOIVKUadf0MX1tGQwDqcaukn2eeeabzOFw2W5UIV22jjIfnPuuz8/nVdfC9VsfhtszSTpmKNw6qMcZU2NiNaQQbuzGNYGM3phF6z3rj4JdMOWMW5FSWFfdRwQWZUr0sWimhjdtUNRklEvF+KoCIBToVHMQipzo/X2tGWMpmYrEgqIJ6uGy3mmuuCpSpZqPGyPOo5p7FRyXyqvXhM6Iqz+OoffieZUS8+azp7je7MY1gYzemEWzsxjRC74kw7G9nfDLeR/mo7FvNp+rm2WSq0nIfNeZMFRp1HXytajyZCjN8fjWezHrgqqIL+5uZJaL4PgO5gBn2Y5WGkQkg4j5qXjP3UfnEmWQUntuMpqToOpd9dmOMjd2YVrCxG9MIncYeEZdHxIMR8UREPB4Rtw3axyLi/ojYO/j/0qUfrjFmVDIC3WkAnyql/CwiLgHw04i4H8BfAniglHJHRNwO4HYAn5nrQBFRCWe8rUSKTJWRUcQNFcShRDOGhRwliigBKCP2sNil+rBopgQ6Pr8S6LhPRpwE6rlWS0txZh4H2QC1QKfmnudDHYfJCJbqXKotU70nkz3Y9dwDuXvPbXycBWW9lVIOlFJ+Nvj5ZQBPApgEcCOA3YNuuwHc1HUsY8zyMa+v3iJiO4B3AHgIwJZSymyc5EEAW86xz60AbgX01zjGmH5IC3QRsQ7AdwF8opQyFBxdZj5byC/4Sil3llJ2lVJ2Zb5XNcYsDak3e0SswYyhf72U8r1B86GImCilHIiICQD1+sfE6tWrsX79+qE29v9UNdVMMAr3UT48+zsqGIP7jLr8rvK32L/KVFNVx8kEw7CPquaDr035kZk5Un/E+Xwq8YOPrXx/Pk6mmo6as0xCiwrY4edRJS9lqgZnfGulNTB8bZkKtG+MoatDzIzqLgBPllK+cNav7gVw8+DnmwHckz6rMaZ3Mm/29wD4CwD/GxGPDtr+FsAdAL4VEbcAeA7Any/JCI0xi0KnsZdSfgzgXHr++xZ3OMaYpcIRdMY0Qq9Zb0D38kZcyQaoK6Mo0YhFu0yVDyXSMEo0YUFK9VEiHgs5atko7qOuledMCXS8n7pWFo1UkJGqCpS5jg0bNgxtX3ppHWDJYq2aRz4/LxcG1MFAKvAmU6lGXSvPtRKQmUzATuZ+ZDI3XUraGFNhYzemEWzsxjRC79VlOYmFEyaOHTtW7ZepHMt9MkEtKhgkE7TAVVeUf678T/Z1VVANVzhVPiL7f8pnzyS1cPiyGo9qW7du3dC2qsrKPjrvA9TVbJQfy9ehglq4Tc0Zn3/UZb0UHIyjfH8eo9JieEzqueJn2NVljTEVNnZjGsHGbkwj2NiNaYRlX/6JBbBRM8oWK302k5nGY1TCjsrdZ9Fq06ZNVR8Wu1RgBS//pOBz8VJL6lxq7pXYxNemgnH4fFu21OUOOKhGnYvFx8y9zwi4WYGO91MBM3x+dR2ZcuiZDD8+F98LC3TGGBu7Ma1gYzemEWzsxjRC72u9da25lclWG3UdtwyZssAsknAkGFBnfQG1aKYywVgkUhlcV1xxxdD2zp07qz4ZgYyPrUSrZ599tmrLRCKy+Kai7FhcUtGKGTGUx5Mp/6z6KGEvs2YeR9CpCD4W6DJZb5nSao6gM8ZU2NiNaQQbuzGN0KvPXkrpDJJQPkemxO4oGUuZbDUVVMM+uvLPx8fHqzb2o1Um2PHjx4e21VJXXHJZVYphn1D5mnwdyq9WATM812qOeD/24YHaR1UZjzzGffv2VX1YB8oEZmWDajLLRnH1HFWSmrPeMlmZCh4jX+tcy6D5zW5MI9jYjWkEG7sxjWBjN6YRei8l3cWo5YJGEeiUQJZZ+5zLUilhS2W0ZdYW56zAgwcPVn1YEFICGZf7uuyyy6o+l19++dD21NRU1UcJdJm1zZhMiaVRy0LxfirIh/uoQCh1r1noVOIbB8yo0llqv1HoKl1lgc4YY2M3phVs7MY0Qq8++6pVqyp/l30QlSCQSWzIrNHNZAJmVMUZ9r1VQotqY/+Kl7UCgEOHDg1tK/+PAz3U/HAyhgqqySwdpHSGTFAR+47KH2afXfm13KZKZPO9zlxH1mfnIBoV5MTnV34zP9dz+dZzwcfhMdtnN8bY2I1pBRu7MY1gYzemEXoX6DhIgwUHtQYWi0tKfJvPOtWzqEAPFhBVUAlnmY2NjVV9lEDHollGkFLzwSKVEq1UdhbDx1b7qKorLHYpUZWFIyWIsUCaERHVvec2FVTDQivfZ0ALdDwmdc8yFXd4jBnROfOcsyDnSjXGGBu7Ma3QaewRcXFE/CQifh4Rj0fE5wbtOyLioYjYFxHfjIj6C2ljzIoh47O/CuDaUsorEbEGwI8j4j8AfBLAF0spd0fEvwK4BcBX5jqQCqrJJFEoH4hh/1v5411VPoA6QEQFjHACjfLZVZINV5jJLOOk/M+uwCSgTsRRSS7btm0b2lYJPSqoJ1NdlnUE5evyfc3oE8rXZr81EwiltJiMrqCeGR6j8sczgT9d+wD1tbJesiCfvcwwq9KsGfwrAK4F8J1B+24AN3UdyxizfKR89ohYHRGPAjgM4H4AvwJwspQy+ydsGsDkkozQGLMopIy9lHKmlPJ2AFMArgFwdfYEEXFrROyJiD3qKxpjTD/MS40vpZwE8CCAdwPYGBGzzuIUgBfOsc+dpZRdpZRdi7WssjFm/nQKdBExDuBUKeVkRKwFcB2Az2PG6D8M4G4ANwO4J3GsziydjBinRD0WTtQfFu6jst5YkFNlmll8U5lh6tj8yUZlvfG1qYo3PMaJiYmqz1VXXTW0vWPHjqoPi1Rq7lUJaBbSMpl5mT/0KmOL51EJnyyIqXPxcZQYlxHtlEB45MiROccD1NeWKXetxLauYJy5BLqMGj8BYHdErMbMJ4FvlVLui4gnANwdEf8I4BEAdyWOZYxZJjqNvZTyGIB3iPanMeO/G2POAxxBZ0wj9L78E/ty7MuoYAP2m0atZsrBFsofZ01B+XEqyYXh6q5A7etydVegrgL71re+tepz9dXDX4a85S1vqfps3759aFv5sawZqISazLWqYBiukssBRer8mSQTBfdRGgprD5lgKaB+jpRPrPx/hp9zNdeZCrj8PLItzLVUmt/sxjSCjd2YRrCxG9MINnZjGmHZBTomEwyTCUhQwg4HRKhzsQCjQny5jxJ71HJLHCCjBKl3vvOdQ9uTk3XKAWenKdGIq87wuu8KJcap62B4+SOFCpjh/ZQ4y/spcZbvtbr3cwlXs4xabptRzxULhOo4vJ8aM8/HfIJ1/GY3phFs7MY0go3dmEbo1WePiMovYT9J+TLsgym/jfdTiSjss6vgC/aTlP+VWf5JVa9hf0rpARzYoQJW9u/fP7SdCVhRVWJZa9i5c2fVZ+vWrVUbz78K/uDrUH4937NMldxM5djMklUqoUW1ZZaVVsE4XcfJaFMq6YWTjjJVhN8YQ7qnMea8xsZuTCPY2I1pBBu7MY3Qq0C3Zs0abN68eaiNA0uU4MCiiBI3uE31YfFNCX2ZyihKyGGU2MTnU8Ewjz322NC2qgKjKtwwmWwxFs1UFuCWLVuqNhaSVMAMj1sFEGWWw+LnQWUh8r1W187XpoKF1LEZJZrx+VUfblPiLF9r5t7znM1VycZvdmMawcZuTCPY2I1phF599gsvvLCqoHLw4MGh7aNHj1b7ZSqVsp82avXOzJLNjKpKw4EvQB0go3zdzLJJHESjtAcOmFF+LOsRyo9UQSTs67/44otVH068OXToUNWH770KDsoES2WWY+bAJ5VgpDQLvh/Kj+bzq3vGc5RZrlsFQvEc8b1Qz9QsfrMb0wg2dmMawcZuTCPY2I1phN6DajhIg4MAVFANC0dKgOGAmUxFERUwwyKNyuhi0UaJikq0YwFGVUbhjC0lrGUyqLiPEp94zfYrr7yy6qPOzyLRsWPHqj7T09ND20qgU8IewwJpJqMtU3FHZSWqY7MgpwQwfmbVM8xzpgKjMgFmC1kc1W92YxrBxm5MI9jYjWkEG7sxjbDsZalYJFPZSJnSOyxIqagyPjeXZAZqQUqVU2Jx5cCBA1UfXrMbqIU9JSKyIKSEpEyJJZ5HtYb7+Pj4nMcFgOeee65qO3HixNC2ulaeIzWPHMGoouMyAh3fR5Wpx/OohEeVMcZRj0og42tV4lsmw4/nSD33XeNxBJ0xxsZuTCvY2I1phN6Xf2K/iP1oFfzBvpyqAsPHVT4Z+6RqeR32k5SvyT6qCqBRGVyMGmNmKSWeM1VumpcbUhlUTz311ND2008/XfVR2YM8/8rXZd9R+dqZZb1GqUKjgmrY91cZjyqjjX1rdX8y5Z35HqmAKu6jxtN1rrmWV/Ob3ZhGsLEb0whpY4+I1RHxSETcN9jeEREPRcS+iPhmRHRXODTGLBvzebPfBuDJs7Y/D+CLpZQrAZwAcMtiDswYs7ikBLqImALwfgD/BOCTMRMNci2Ajw267AbwDwC+Mtdxzpw5UwUcsOCiBLrMOt4s4mXW41bBDyx4KKGN25QoogJEuJ8KgGARUR2H50iJXxz4oq6V5ygjaqrzq9JdLNopQYwFORYVVZvKVOQAIvUM8dwrkVfdD27LZKKpwBs+nxL6MiIek8nunCX7Zv8SgE8DmL3yTQBOllJm7+g0gLqolzFmxdBp7BHxAQCHSyk/HeUEEXFrROyJiD3qqwRjTD9kPsa/B8CHIuIGABcDWA/gywA2RsQFg7f7FIAX1M6llDsB3AkAExMT9Wc5Y0wvdBp7KeWzAD4LABHxXgB/U0r5eER8G8CHAdwN4GYA9yzGgDKBFcr/y/jD7EeqYBD2rZT/xedS41Go5Bxm06ZNQ9vKR+WgEeWjsj+e8UfVdahkIb6OzHJHisw699ymxsPXr+aZ72MmEGjU/ZSvPcozo/xxbsuUTJ9lId+zfwYzYt0+zPjwdy3gWMaYJWZe4bKllB8C+OHg56cBXLP4QzLGLAWOoDOmEWzsxjRCr1lvq1atqgI3WExRggOLEir4g4UUVQmExZXMeuCqD3+FmBGjFEqA4WOp4CAOYmFRD6jFLyVaZUSiTEWXTCUhFRzEwTBbt26t+vC1qco9PEYlkPHzoYJqMploKsOQ2zIi3qgCHd9H7jNXkI3f7MY0go3dmEawsRvTCL1Xl+VKqOxjKF8mE4zCfVQwDCcfZBIWlI/GfdSYle+UCYDgJBvls3NyivKr+fwqOIf9erWMVGa5IXV+PpZKluEqsKoqLAfVqKQf9ocz9zUTLKX6jXrsTBVYfj4yfTIJX7P4zW5MI9jYjWkEG7sxjWBjN6YRehfoWMxhcUsFJHAfJX6xIKTEJj636qMEuS5GFeMULPZk1jBX1XQ2bNgwtK0y47iPqkqjRCIOkFH7sSCnstU4YIbHA9TjzmQqZgJmMlmA6nyZjMtMhRklOvNzpPp0BaE5qMYYY2M3phVs7MY0Qu+JMBxUw36S8nXZb8pU1FSJFxyQoZZEYj8+E9Cj/LhMcozy7TKBFXxsdR0nT54c2lY+MyewKL9ezXWmmiv746oP+/UqOIfnQ9UxzAS+ZBJRMgksmWcvk8CS0XTUcbr0K/vsxhgbuzGtYGM3phFs7MY0Qu8CHQdgsHChBKlMkAILF5mgGgXvp8QeFkEygUDn6sfwGFVWUyaAiNvUcTJik8pWY/Ftamqq6qPKQjOZez+KQJYhk+GWPT/PdXY5sK5zZao2WaAzxlTY2I1pBBu7MY2w7D57JohllIAI5aNym6p6wuNTx+GAlYyvp/qphA0OOlI6wyh+PR9X9VH+nqoeMzk5Oec2kAtgylQO4jalT/C9zwRCZZesyiRh8fxnKv5kE3G6xsPYZzfG2NiNaQUbuzGNYGM3phEiu7b4opws4giA5wBcBuBobydeHM7HMQPn57g95tH5g1LKuPpFr8b+xkkj9pRSdvV+4gVwPo4ZOD/H7TEvDf4Yb0wj2NiNaYTlMvY7l+m8C+F8HDNwfo7bY14ClsVnN8b0jz/GG9MIvRt7RFwfEU9FxL6IuL3v82eIiK9GxOGI+MVZbWMRcX9E7B38352w3SMRcXlEPBgRT0TE4xFx26B9xY47Ii6OiJ9ExM8HY/7coH1HRDw0eEa+GRHdhQh6JiJWR8QjEXHfYHvFj7lXY4+I1QD+BcCfAXgbgI9GxNv6HEOSrwG4ntpuB/BAKWUngAcG2yuJ0wA+VUp5G4B3Afirwdyu5HG/CuDaUsofA3g7gOsj4l0APg/gi6WUKwGcAHDL8g3xnNwG4Mmztlf8mPt+s18DYF8p5elSymsA7gZwY89j6KSU8iMAx6n5RgC7Bz/vBnBTn2PqopRyoJTys8HPL2PmQZzECh53mWE2HW7N4F8BcC2A7wzaV9SYASAipgC8H8C/DbYDK3zMQP/GPglg/1nb04O284EtpZQDg58PAqhzP1cIEbEdwDsAPIQVPu7Bx+FHARwGcD+AXwE4WUqZzTddic/IlwB8GsBsTuomrPwxW6AbhTLzFcaK/BojItYB+C6AT5RSXjr7dytx3KWUM6WUtwOYwswnv6uXd0RzExEfAHC4lPLT5R7LfOm1eAWAFwBcftb21KDtfOBQREyUUg5ExARm3kQriohYgxlD/3op5XuD5hU/bgAopZyMiAcBvBvAxoi4YPCmXGnPyHsAfCgibgBwMYD1AL6MlT1mAP2/2R8GsHOgXF4I4CMA7u15DKNyL4CbBz/fDOCeZRxLxcBvvAvAk6WUL5z1qxU77ogYj4iNg5/XArgOM1rDgwA+POi2osZcSvlsKWWqlLIdM8/vf5dSPo4VPOY3KKX0+g/ADQB+iRnf7O/6Pn9yjN8AcADAKcz4X7dgxi97AMBeAP8FYGy5x0lj/hPMfER/DMCjg383rORxA/gjAI8MxvwLAH8/aP9DAD8BsA/AtwFctNxjPcf43wvgvvNlzI6gM6YRLNAZ0wg2dmMawcZuTCPY2I1pBBu7MY1gYzemEWzsxjSCjd2YRvg/VUn9vdDNuUwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ImageDataGenerator is an iterator.\n",
    "\n",
    "# specify the batch size hyperparameter. You can experiment with different batch sizes\n",
    "batch_size = 16\n",
    "\n",
    "# create the ImageDataGenerator with rescaling that will generate batched tensors representing images with real-time data augmentation\n",
    "# use at least two of the augmentation strategies. For example, fill_mode='nearest'\n",
    "# please refer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# (3)\n",
    "train_img_gen = ImageDataGenerator(\n",
    "    brightness_range=(0.5,1),\n",
    "    fill_mode='nearest',\n",
    "    horizontal_flip=True,\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the \"ImageDataGenerator\" instance to link the image folder and the dataframe.\n",
    "# also include the, batch size, image size and the seed.\n",
    "# make sure to include the following arguments\n",
    "# color_mode='grayscale', class_mode='multi_output'\n",
    "# please refer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# (5)\n",
    "# TODO\n",
    "\n",
    "train_img_flow = train_img_gen.flow_from_dataframe(\n",
    "    dfTrain,\n",
    "    directory=r\"./data/images/train/\",\n",
    "    x_col= \"img_name\",\n",
    "    y_col= [\"age\", \"ethnicity\", \"gender\"],\n",
    "    batch_size=16,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='multi_output',\n",
    "    target_size=(48, 48),\n",
    "    seed=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# similarly, create an ImageDataGenerator for the validation dataset and make sure not to use any of the augmentation strategies except rescaling the image\n",
    "# (2)\n",
    "val_img_gen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the \"ImageDataGenerator\" instance with the same arguments as above\n",
    "# make sure to specify the following arguments:\n",
    "# class_mode='multi_output', color_mode='grayscale', shuffle=False\n",
    "# (5)\n",
    "\n",
    "test_img_flow = val_img_gen.flow_from_dataframe(\n",
    "    dfTest,\n",
    "    directory=r\"./data/images/test/\",\n",
    "    x_col= \"img_name\",\n",
    "    y_col= [\"age\", \"ethnicity\", \"gender\"],\n",
    "    batch_size=16,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='multi_output',\n",
    "    shuffle=False,\n",
    "    target_size=(48, 48),\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the val_img_gen instance to link the test dataframe and the test data folder\n",
    "# In addition, make sure to specify the following arguments\n",
    "# class_mode='multi_output', color_mode='grayscale', shuffle=False\n",
    "# (5)\n",
    "val_img_flow = val_img_gen.flow_from_dataframe(\n",
    "    dfVal,\n",
    "    directory=r\"./data/images/val/\",\n",
    "    x_col= \"img_name\",\n",
    "    y_col= [\"age\", \"ethnicity\", \"gender\"],\n",
    "    batch_size=16,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='multi_output',\n",
    "    shuffle=False,\n",
    "    target_size=(48, 48),\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# enumerate through the validation data generator created above and plot first grayscale image \n",
    "# (2)\n",
    "for i, element in enumerate(val_img_flow):\n",
    "    plt.imshow(element[0][0],cmap=plt.cm.binary)\n",
    "    plt.show()    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model (44/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_22 (InputLayer)          [(None, 48, 48, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 46, 46, 32)   320         ['input_22[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_42 (MaxPooling2D  (None, 23, 23, 32)  0           ['conv2d_63[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 21, 21, 64)   18496       ['max_pooling2d_42[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_43 (MaxPooling2D  (None, 10, 10, 64)  0           ['conv2d_64[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 8, 8, 128)    73856       ['max_pooling2d_43[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_24 (Flatten)           (None, 8192)         0           ['conv2d_65[0][0]']              \n",
      "                                                                                                  \n",
      " Dense_Layer_1 (Dense)          (None, 128)          1048704     ['flatten_24[0][0]']             \n",
      "                                                                                                  \n",
      " Dense_Layer_2 (Dense)          (None, 128)          1048704     ['flatten_24[0][0]']             \n",
      "                                                                                                  \n",
      " Dense_Layer_3 (Dense)          (None, 128)          1048704     ['flatten_24[0][0]']             \n",
      "                                                                                                  \n",
      " Dense_Age (Dense)              (None, 1)            129         ['Dense_Layer_1[0][0]']          \n",
      "                                                                                                  \n",
      " Dense_Ethnicity (Dense)        (None, 5)            645         ['Dense_Layer_2[0][0]']          \n",
      "                                                                                                  \n",
      " Dense_Gender (Dense)           (None, 1)            129         ['Dense_Layer_3[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,239,687\n",
      "Trainable params: 3,239,687\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "939/939 [============================>.] - ETA: 0s - loss: 0.7454 - Dense_Age_loss: 13.8710 - Dense_Ethnicity_loss: 1.0251 - Dense_Gender_loss: 0.4379 - Dense_Age_mean_absolute_error: 13.8710 - Dense_Age_Accuracy: 0.0422 - Dense_Age_binary_accuracy: 0.0422 - Dense_Ethnicity_mean_absolute_error: 1.2414 - Dense_Ethnicity_Accuracy: 0.6201 - Dense_Ethnicity_binary_accuracy: 0.3912 - Dense_Gender_mean_absolute_error: 0.2874 - Dense_Gender_Accuracy: 0.7893 - Dense_Gender_binary_accuracy: 0.7893WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "939/939 [==============================] - 43s 45ms/step - loss: 0.7452 - Dense_Age_loss: 13.8662 - Dense_Ethnicity_loss: 1.0248 - Dense_Gender_loss: 0.4378 - Dense_Age_mean_absolute_error: 13.8662 - Dense_Age_Accuracy: 0.0422 - Dense_Age_binary_accuracy: 0.0422 - Dense_Ethnicity_mean_absolute_error: 1.2416 - Dense_Ethnicity_Accuracy: 0.6201 - Dense_Ethnicity_binary_accuracy: 0.3911 - Dense_Gender_mean_absolute_error: 0.2873 - Dense_Gender_Accuracy: 0.7894 - Dense_Gender_binary_accuracy: 0.7894 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "939/939 [============================>.] - ETA: 0s - loss: 0.5498 - Dense_Age_loss: 10.8955 - Dense_Ethnicity_loss: 0.7616 - Dense_Gender_loss: 0.3162 - Dense_Age_mean_absolute_error: 10.8955 - Dense_Age_Accuracy: 0.0385 - Dense_Age_binary_accuracy: 0.0385 - Dense_Ethnicity_mean_absolute_error: 1.2416 - Dense_Ethnicity_Accuracy: 0.7320 - Dense_Ethnicity_binary_accuracy: 0.3835 - Dense_Gender_mean_absolute_error: 0.1963 - Dense_Gender_Accuracy: 0.8628 - Dense_Gender_binary_accuracy: 0.8628WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "939/939 [==============================] - 42s 45ms/step - loss: 0.5496 - Dense_Age_loss: 10.8928 - Dense_Ethnicity_loss: 0.7615 - Dense_Gender_loss: 0.3160 - Dense_Age_mean_absolute_error: 10.8928 - Dense_Age_Accuracy: 0.0385 - Dense_Age_binary_accuracy: 0.0385 - Dense_Ethnicity_mean_absolute_error: 1.2416 - Dense_Ethnicity_Accuracy: 0.7321 - Dense_Ethnicity_binary_accuracy: 0.3835 - Dense_Gender_mean_absolute_error: 0.1962 - Dense_Gender_Accuracy: 0.8629 - Dense_Gender_binary_accuracy: 0.8629 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "940/939 [==============================] - ETA: 0s - loss: 0.4953 - Dense_Age_loss: 9.8850 - Dense_Ethnicity_loss: 0.6856 - Dense_Gender_loss: 0.2852 - Dense_Age_mean_absolute_error: 9.8850 - Dense_Age_Accuracy: 0.0369 - Dense_Age_binary_accuracy: 0.0369 - Dense_Ethnicity_mean_absolute_error: 1.2416 - Dense_Ethnicity_Accuracy: 0.7609 - Dense_Ethnicity_binary_accuracy: 0.3819 - Dense_Gender_mean_absolute_error: 0.1718 - Dense_Gender_Accuracy: 0.8776 - Dense_Gender_binary_accuracy: 0.8776WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "939/939 [==============================] - 42s 44ms/step - loss: 0.4953 - Dense_Age_loss: 9.8850 - Dense_Ethnicity_loss: 0.6856 - Dense_Gender_loss: 0.2852 - Dense_Age_mean_absolute_error: 9.8850 - Dense_Age_Accuracy: 0.0369 - Dense_Age_binary_accuracy: 0.0369 - Dense_Ethnicity_mean_absolute_error: 1.2416 - Dense_Ethnicity_Accuracy: 0.7609 - Dense_Ethnicity_binary_accuracy: 0.3819 - Dense_Gender_mean_absolute_error: 0.1718 - Dense_Gender_Accuracy: 0.8776 - Dense_Gender_binary_accuracy: 0.8776 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "940/939 [==============================] - ETA: 0s - loss: 0.4626 - Dense_Age_loss: 9.2021 - Dense_Ethnicity_loss: 0.6404 - Dense_Gender_loss: 0.2664 - Dense_Age_mean_absolute_error: 9.2021 - Dense_Age_Accuracy: 0.0361 - Dense_Age_binary_accuracy: 0.0361 - Dense_Ethnicity_mean_absolute_error: 1.2416 - Dense_Ethnicity_Accuracy: 0.7757 - Dense_Ethnicity_binary_accuracy: 0.3815 - Dense_Gender_mean_absolute_error: 0.1599 - Dense_Gender_Accuracy: 0.8854 - Dense_Gender_binary_accuracy: 0.8854WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "939/939 [==============================] - 42s 44ms/step - loss: 0.4626 - Dense_Age_loss: 9.2021 - Dense_Ethnicity_loss: 0.6404 - Dense_Gender_loss: 0.2664 - Dense_Age_mean_absolute_error: 9.2021 - Dense_Age_Accuracy: 0.0361 - Dense_Age_binary_accuracy: 0.0361 - Dense_Ethnicity_mean_absolute_error: 1.2416 - Dense_Ethnicity_Accuracy: 0.7757 - Dense_Ethnicity_binary_accuracy: 0.3815 - Dense_Gender_mean_absolute_error: 0.1599 - Dense_Gender_Accuracy: 0.8854 - Dense_Gender_binary_accuracy: 0.8854 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "940/939 [==============================] - ETA: 0s - loss: 0.4387 - Dense_Age_loss: 8.7285 - Dense_Ethnicity_loss: 0.6089 - Dense_Gender_loss: 0.2511 - Dense_Age_mean_absolute_error: 8.7285 - Dense_Age_Accuracy: 0.0351 - Dense_Age_binary_accuracy: 0.0351 - Dense_Ethnicity_mean_absolute_error: 1.2416 - Dense_Ethnicity_Accuracy: 0.7908 - Dense_Ethnicity_binary_accuracy: 0.3813 - Dense_Gender_mean_absolute_error: 0.1503 - Dense_Gender_Accuracy: 0.8918 - Dense_Gender_binary_accuracy: 0.8918WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "939/939 [==============================] - 42s 44ms/step - loss: 0.4387 - Dense_Age_loss: 8.7285 - Dense_Ethnicity_loss: 0.6089 - Dense_Gender_loss: 0.2511 - Dense_Age_mean_absolute_error: 8.7285 - Dense_Age_Accuracy: 0.0351 - Dense_Age_binary_accuracy: 0.0351 - Dense_Ethnicity_mean_absolute_error: 1.2416 - Dense_Ethnicity_Accuracy: 0.7908 - Dense_Ethnicity_binary_accuracy: 0.3813 - Dense_Gender_mean_absolute_error: 0.1503 - Dense_Gender_Accuracy: 0.8918 - Dense_Gender_binary_accuracy: 0.8918 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "940/939 [==============================] - ETA: 0s - loss: 0.4137 - Dense_Age_loss: 8.3683 - Dense_Ethnicity_loss: 0.5704 - Dense_Gender_loss: 0.2402 - Dense_Age_mean_absolute_error: 8.3683 - Dense_Age_Accuracy: 0.0343 - Dense_Age_binary_accuracy: 0.0343 - Dense_Ethnicity_mean_absolute_error: 1.2416 - Dense_Ethnicity_Accuracy: 0.7970 - Dense_Ethnicity_binary_accuracy: 0.3813 - Dense_Gender_mean_absolute_error: 0.1419 - Dense_Gender_Accuracy: 0.9000 - Dense_Gender_binary_accuracy: 0.9000WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "939/939 [==============================] - 42s 45ms/step - loss: 0.4137 - Dense_Age_loss: 8.3683 - Dense_Ethnicity_loss: 0.5704 - Dense_Gender_loss: 0.2402 - Dense_Age_mean_absolute_error: 8.3683 - Dense_Age_Accuracy: 0.0343 - Dense_Age_binary_accuracy: 0.0343 - Dense_Ethnicity_mean_absolute_error: 1.2416 - Dense_Ethnicity_Accuracy: 0.7970 - Dense_Ethnicity_binary_accuracy: 0.3813 - Dense_Gender_mean_absolute_error: 0.1419 - Dense_Gender_Accuracy: 0.9000 - Dense_Gender_binary_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "939/939 [============================>.] - ETA: 0s - loss: 0.4007 - Dense_Age_loss: 8.1340 - Dense_Ethnicity_loss: 0.5535 - Dense_Gender_loss: 0.2316 - Dense_Age_mean_absolute_error: 8.1340 - Dense_Age_Accuracy: 0.0339 - Dense_Age_binary_accuracy: 0.0339 - Dense_Ethnicity_mean_absolute_error: 1.2417 - Dense_Ethnicity_Accuracy: 0.8109 - Dense_Ethnicity_binary_accuracy: 0.3813 - Dense_Gender_mean_absolute_error: 0.1369 - Dense_Gender_Accuracy: 0.9034 - Dense_Gender_binary_accuracy: 0.9034WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "939/939 [==============================] - 42s 44ms/step - loss: 0.4006 - Dense_Age_loss: 8.1356 - Dense_Ethnicity_loss: 0.5535 - Dense_Gender_loss: 0.2315 - Dense_Age_mean_absolute_error: 8.1356 - Dense_Age_Accuracy: 0.0339 - Dense_Age_binary_accuracy: 0.0339 - Dense_Ethnicity_mean_absolute_error: 1.2416 - Dense_Ethnicity_Accuracy: 0.8109 - Dense_Ethnicity_binary_accuracy: 0.3812 - Dense_Gender_mean_absolute_error: 0.1368 - Dense_Gender_Accuracy: 0.9035 - Dense_Gender_binary_accuracy: 0.9035 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "939/939 [============================>.] - ETA: 0s - loss: 0.3852 - Dense_Age_loss: 7.9687 - Dense_Ethnicity_loss: 0.5268 - Dense_Gender_loss: 0.2278 - Dense_Age_mean_absolute_error: 7.9687 - Dense_Age_Accuracy: 0.0324 - Dense_Age_binary_accuracy: 0.0324 - Dense_Ethnicity_mean_absolute_error: 1.2412 - Dense_Ethnicity_Accuracy: 0.8213 - Dense_Ethnicity_binary_accuracy: 0.3815 - Dense_Gender_mean_absolute_error: 0.1334 - Dense_Gender_Accuracy: 0.9055 - Dense_Gender_binary_accuracy: 0.9055WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "939/939 [==============================] - 42s 44ms/step - loss: 0.3853 - Dense_Age_loss: 7.9688 - Dense_Ethnicity_loss: 0.5271 - Dense_Gender_loss: 0.2276 - Dense_Age_mean_absolute_error: 7.9688 - Dense_Age_Accuracy: 0.0323 - Dense_Age_binary_accuracy: 0.0323 - Dense_Ethnicity_mean_absolute_error: 1.2416 - Dense_Ethnicity_Accuracy: 0.8212 - Dense_Ethnicity_binary_accuracy: 0.3813 - Dense_Gender_mean_absolute_error: 0.1334 - Dense_Gender_Accuracy: 0.9056 - Dense_Gender_binary_accuracy: 0.9056 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "940/939 [==============================] - ETA: 0s - loss: 0.3697 - Dense_Age_loss: 7.8379 - Dense_Ethnicity_loss: 0.5047 - Dense_Gender_loss: 0.2190 - Dense_Age_mean_absolute_error: 7.8379 - Dense_Age_Accuracy: 0.0326 - Dense_Age_binary_accuracy: 0.0326 - Dense_Ethnicity_mean_absolute_error: 1.2416 - Dense_Ethnicity_Accuracy: 0.8267 - Dense_Ethnicity_binary_accuracy: 0.3799 - Dense_Gender_mean_absolute_error: 0.1276 - Dense_Gender_Accuracy: 0.9078 - Dense_Gender_binary_accuracy: 0.9078WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "939/939 [==============================] - 42s 44ms/step - loss: 0.3697 - Dense_Age_loss: 7.8379 - Dense_Ethnicity_loss: 0.5047 - Dense_Gender_loss: 0.2190 - Dense_Age_mean_absolute_error: 7.8379 - Dense_Age_Accuracy: 0.0326 - Dense_Age_binary_accuracy: 0.0326 - Dense_Ethnicity_mean_absolute_error: 1.2416 - Dense_Ethnicity_Accuracy: 0.8267 - Dense_Ethnicity_binary_accuracy: 0.3799 - Dense_Gender_mean_absolute_error: 0.1276 - Dense_Gender_Accuracy: 0.9078 - Dense_Gender_binary_accuracy: 0.9078 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "940/939 [==============================] - ETA: 0s - loss: 0.3561 - Dense_Age_loss: 7.7078 - Dense_Ethnicity_loss: 0.4898 - Dense_Gender_loss: 0.2069 - Dense_Age_mean_absolute_error: 7.7078 - Dense_Age_Accuracy: 0.0334 - Dense_Age_binary_accuracy: 0.0334 - Dense_Ethnicity_mean_absolute_error: 1.2416 - Dense_Ethnicity_Accuracy: 0.8322 - Dense_Ethnicity_binary_accuracy: 0.3800 - Dense_Gender_mean_absolute_error: 0.1214 - Dense_Gender_Accuracy: 0.9140 - Dense_Gender_binary_accuracy: 0.9140WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,Dense_Age_loss,Dense_Ethnicity_loss,Dense_Gender_loss,Dense_Age_mean_absolute_error,Dense_Age_Accuracy,Dense_Age_binary_accuracy,Dense_Ethnicity_mean_absolute_error,Dense_Ethnicity_Accuracy,Dense_Ethnicity_binary_accuracy,Dense_Gender_mean_absolute_error,Dense_Gender_Accuracy,Dense_Gender_binary_accuracy,lr\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "939/939 [==============================] - 42s 44ms/step - loss: 0.3561 - Dense_Age_loss: 7.7078 - Dense_Ethnicity_loss: 0.4898 - Dense_Gender_loss: 0.2069 - Dense_Age_mean_absolute_error: 7.7078 - Dense_Age_Accuracy: 0.0334 - Dense_Age_binary_accuracy: 0.0334 - Dense_Ethnicity_mean_absolute_error: 1.2416 - Dense_Ethnicity_Accuracy: 0.8322 - Dense_Ethnicity_binary_accuracy: 0.3800 - Dense_Gender_mean_absolute_error: 0.1214 - Dense_Gender_Accuracy: 0.9140 - Dense_Gender_binary_accuracy: 0.9140 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x291b5de2740>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify the model input with the required shape \n",
    "# (1)\n",
    "model_input = keras.Input(shape=(48, 48, 1)) \n",
    "\n",
    "# The shared layers\n",
    "# Include at least one Conv2D layer, MaxPooling2D layer and a Flatten layer\n",
    "# you can have as many layers as possible, but make sure not to overfit your model using the training data\n",
    "# (10)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(model_input) \n",
    "x = layers.MaxPooling2D(pool_size=2)(x) \n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x) \n",
    "x = layers.MaxPooling2D(pool_size=2)(x) \n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x) \n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Task specific layers\n",
    "# Include at least one Dense layer as a task specific layer before generating the output for age\n",
    "\n",
    "# (2)\n",
    "\n",
    "dense_layer_1 = layers.Dense(128, activation=\"relu\", name=\"Dense_Layer_1\")(x) \n",
    "\n",
    "#model = keras.Model(inputs=model_input, outputs=dense_layer)\n",
    "\n",
    "# Include the age output and make sure to include the following arguments\n",
    "# activation='linear', name='xxx'(any name)\n",
    "# make sure to name your output layers so that different metrics to be used can be linked accordingly\n",
    "# please note that the age prediction is a regression task\n",
    "\n",
    "# (2)\n",
    "age_output = layers.Dense(1, activation=\"linear\", name=\"Dense_Age\")(dense_layer_1)\n",
    "#age_output = layers.Flatten()(age_output)\n",
    "\n",
    "# Similar to above, specify one or more Dense layers as task specific layers for ethnicity prediction\n",
    "# (2)\n",
    "dense_layer_2 = layers.Dense(128, activation=\"relu\", name=\"Dense_Layer_2\")(x) \n",
    "\n",
    "# Include the ethnicity output that uses the task specific output from the layer above\n",
    "# please note that the ethnicity prediction is a multi-class classification task\n",
    "# (2)\n",
    "\n",
    "eth_output = layers.Dense(5, activation=\"softmax\", name=\"Dense_Ethnicity\")(dense_layer_2)\n",
    "#eth_output = layers.Flatten()(eth_output)\n",
    "\n",
    "# Similar to above, specify one or more Dense layers as task specific layers for gender prediction\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "dense_layer_3 = layers.Dense(128, activation=\"sigmoid\", name=\"Dense_Layer_3\")(x) \n",
    "\n",
    "# Include the gender output that uses the task specific output from the layer above\n",
    "# please note that the ethnicity prediction is a binary classification task\n",
    "\n",
    "# (2)\n",
    "\n",
    "gen_output = layers.Dense(1, activation=\"sigmoid\", name=\"Dense_Gender\")(dense_layer_3)\n",
    "#gen_output = layers.Flatten()(gen_output)\n",
    "\n",
    "# create the model with the required input and the outputs.\n",
    "# pelase make sure that the outputs can be included in a list and make sure to keep note of the order\n",
    "# (3)\n",
    "\n",
    "model = keras.Model(inputs=model_input, outputs=[age_output, eth_output, gen_output])\n",
    "\n",
    "\n",
    "# print the model summary\n",
    "# (0.5)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Instantiate the optimizer with the learning rate. You can start with the learning rate 1e-3(0.001).\n",
    "# Both the optimizer and the learning rate are hyperparameters that you can finetune\n",
    "# For example, you can start with the \"RMSprop\" optimizer\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "# specify the losses to be used for each task: age, ethnicity and gender prediction \n",
    "# (0.5)\n",
    "losses = ['MeanAbsoluteError', 'sparse_categorical_crossentropy', 'binary_crossentropy']\n",
    "\n",
    "# compile the model with the optimizer, loss, loss_weights and the metrics for each task\n",
    "# apply the following weights to the losses to balance the contribution of each loss to the total loss\n",
    "# loss_weights=[0.001, 0.5, 0.5]\n",
    "# please remember to use the relevant metric for each task by assigning it to the correct output\n",
    "# (2)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=losses, metrics=['MeanAbsoluteError', 'Accuracy' , 'BinaryAccuracy'], loss_weights=[0.001, 0.5, 0.5])\n",
    "\n",
    "# Define the callbacks\n",
    "# EarlyStopping: monitor the validation loss while waiting for 3 epochs before stopping\n",
    "# can restore the best weights\n",
    "# (2)\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# ModelCheckpoint\n",
    "# monitor validation loss and save the best model weights\n",
    "# (2)\n",
    "\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(\n",
    "    filepath='./models/checkpoints',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "# Initiallize TensorBoard\n",
    "# (2)\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir='./logs/tensorlog'\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau\n",
    "# reduce the learning rate by a factor of 0.1 after waiting for 2 epochs while monitoring validation loss\n",
    "# specify a minimum learning rate to be used\n",
    "# TODO\n",
    "# (2)\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# fit the model with training and validation generators\n",
    "# In addition please specify the following arguments\n",
    "# steps_per_epoch=len(df_train)/batch_size\n",
    "# validation_steps=len(df_val)/batch_size\n",
    "# (5)\n",
    "\n",
    "model.fit(\n",
    "    x=train_img_flow,\n",
    "    epochs=10, \n",
    "    batch_size=64, \n",
    "    callbacks=[reduce_lr, early_stop, checkpoints, tensorboard],\n",
    "    steps_per_epoch=len(dfTrain)/batch_size,\n",
    "    validation_steps=len(dfVal)/batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions on test data (14/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/294 [==============================] - 3s 10ms/step - loss: 0.4710 - Dense_Age_loss: 8.1779 - Dense_Ethnicity_loss: 0.6592 - Dense_Gender_loss: 0.2665 - Dense_Age_mean_absolute_error: 8.1779 - Dense_Age_Accuracy: 0.0200 - Dense_Age_binary_accuracy: 0.0200 - Dense_Ethnicity_mean_absolute_error: 1.2422 - Dense_Ethnicity_Accuracy: 0.7802 - Dense_Ethnicity_binary_accuracy: 0.3826 - Dense_Gender_mean_absolute_error: 0.1372 - Dense_Gender_Accuracy: 0.8878 - Dense_Gender_binary_accuracy: 0.8878\n",
      "\n",
      "Dense_Gender_Accuracy:\n",
      "0.8877768516540527\n",
      "\n",
      "\n",
      "Dense_Gender_Binary_Accuracy:\n",
      "0.8877768516540527\n"
     ]
    }
   ],
   "source": [
    "# evaluate the trained model using the test generator\n",
    "# print only the test accuracy for ethnicity and gender predictions\n",
    "(4)\n",
    "test_evals = model.evaluate(\n",
    "    x=test_img_flow\n",
    ")\n",
    "print(\"\\nDense_Gender_Accuracy:\")\n",
    "print(test_evals[11])\n",
    "print(\"\\n\\nDense_Gender_Binary_Accuracy:\")\n",
    "print(test_evals[12])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/294 [==============================] - 3s 9ms/step\n",
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\adamj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\adamj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# generate predictions using the test generator\n",
    "(2)\n",
    "predictions = model.predict(\n",
    "    x=test_img_flow,\n",
    "    batch_size=64, \n",
    "    callbacks=[reduce_lr, early_stop, checkpoints, tensorboard],\n",
    "    verbose=1\n",
    "    )\n",
    "\n",
    "\n",
    "# extract the ethnicity predictions\n",
    "(2)\n",
    "\n",
    "eth_pred_lbls = np.argmax(predictions[1], axis=1)\n",
    "\n",
    "\n",
    "# print the classification report for predicting ethnicity\n",
    "(2)\n",
    "\n",
    "eth_cr = classification_report(dfTest['ethnicity'].values.ravel(), eth_pred_lbls)\n",
    "print(eth_cr)\n",
    "\n",
    "# extract the gender predictions where probabilities above 0.5 are considered class 1 and if not, class 0\n",
    "#TODO\n",
    "(2)\n",
    "\n",
    "gen_pred_lbls = np.argmax(predictions[2], axis=1)\n",
    "print(np.unique(gen_pred_lbls))\n",
    "\n",
    "# print the classification report for predicting gender\n",
    "(2)\n",
    "gen_cr = classification_report(dfTest['gender'].values.ravel(), gen_pred_lbls)\n",
    "print(gen_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Present prediction results on test data(5/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present your findings for 5 different runs by fine-tuning the hyperparameters. The results table must contain the following fields\n",
    "- A minimum of 5 hyperparameters that you have fine-tuned\n",
    "- Mean absolute error for age\n",
    "- Accuracy for ethnicity prediction\n",
    "- Accuracy for gender prediction\n",
    "Please use a table format similar to the one mentioned below when presenting the results.\n",
    "\n",
    "| Hyperparameters | Age(MAE) | Ethnicity(Accuracy)| Gender(Accuracy) |\n",
    "|-----------------|----------|--------------------|------------------|\n",
    "|                 |          |                    |                  |\n",
    "|                 |          |                    |                  |\n",
    "|                 |          |                    |                  |\n",
    "|                 |          |                    |                  |\n",
    "|                 |          |                    |                  |\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcacb1169d0317e97b762e0c1a317633c556cb59cb1017287d7e24b96b766e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
