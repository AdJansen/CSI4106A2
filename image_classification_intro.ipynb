{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 15976), started 1 day, 23:22:07 ago. (Use '!kill 15976' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "import datetime\n",
    "import os\n",
    "# for reproducibility purposes\n",
    "np.random.seed(42)\n",
    "python_random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# load tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directories():\n",
    "    d = datetime.datetime.today()\n",
    "    timestamp = d.strftime('%Y%m%d_%H%M%S')\n",
    "    # folder to store the tensorboard logs\n",
    "    tensorlog_folder = os.path.join(os.path.curdir, 'logs', timestamp)\n",
    "    # folder to store the trained models\n",
    "    checkpoint_folder = os.path.join(os.path.curdir, 'models', timestamp)\n",
    "\n",
    "    os.mkdir(tensorlog_folder)\n",
    "    os.mkdir(checkpoint_folder)\n",
    "\n",
    "    return checkpoint_folder, tensorlog_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# 0: T-shirt/top, 1: Trouser, 2: Pullover, 3: Dress, 4: Coat, 5: Sandal, 6: Shirt, 7: Sneaker, 8: Bag, 9: Ankle boot\n",
    "data = datasets.fashion_mnist\n",
    "(train_imgs, train_lbls), (test_imgs, test_lbls) = data.load_data()\n",
    "\n",
    "print(train_imgs.shape)\n",
    "print(test_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({9: 6000, 0: 6000, 3: 6000, 2: 6000, 7: 6000, 5: 6000, 1: 6000, 6: 6000, 4: 6000, 8: 6000})\n",
      "Counter({9: 1000, 2: 1000, 1: 1000, 6: 1000, 4: 1000, 5: 1000, 7: 1000, 3: 1000, 8: 1000, 0: 1000})\n"
     ]
    }
   ],
   "source": [
    "# normalize the image data\n",
    "train_imgs = train_imgs.astype('float32') / 255\n",
    "test_imgs = test_imgs.astype('float32') / 255\n",
    "\n",
    "print(Counter(train_lbls))\n",
    "print(Counter(test_lbls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAARlUlEQVR4nO3dbWxVZbYH8P+Sl4pQlJdaXgbtDDEao14gJ+TGwQlkdIJEghjUIXHkJnrhAyYQJ4J6lfGLibm5zGQ+GAyjCHMzQAZRwYToaEMkE+OEA0FKURERLU0pxfLSEigtrPuhG2/Fs9dT9z7n7CPr/0uanp519tmrh/7Z7Xn2sx9RVRDRle+qrBsgovJg2ImcYNiJnGDYiZxg2ImcGFjOnY0ePVrr6urKuUsiVw4fPozjx49LoVqqsIvITAB/BjAAwKuq+pL1+Lq6OuTz+TS7JCJDLpeLrSX+NV5EBgB4GcC9AG4FMF9Ebk36fERUWmn+Zp8K4KCqHlLV8wA2AphTnLaIqNjShH08gKY+Xx+J7vseEVkoInkRybe1taXYHRGlUfJ341V1tarmVDVXU1NT6t0RUYw0YW8GMKHP1z+L7iOiCpQm7DsB3CQiPxeRwQB+C2BrcdoiomJLPPSmqj0i8gSA99A79LZGVRuL1hkRFVWqcXZV3QZgW5F6IaIS4umyRE4w7EROMOxETjDsRE4w7EROMOxETpR1PjtVnrRXFxYpOHX6J++RRx4x608++aRZnzJlilnv6uqKrVVVVZnbJsUjO5ETDDuREww7kRMMO5ETDDuREww7kRMceiuD0PBW2uEr6/lDzx2qh3ov5ffW3d1t1gcNGmTWGxoaYmvz5s0ztz1w4IBZ7+zsNOtvv/22Wc9iyJJHdiInGHYiJxh2IicYdiInGHYiJxh2IicYdiInOM5eBmnHstM8/4ULF1I998WLF816T0+PWR8yZEji5w6No+/YscOsz507N7Y2ePBgc9tbbrnFrL/88stmPST0vZUCj+xETjDsRE4w7EROMOxETjDsRE4w7EROMOxETnCcvQKUcm7zgAEDSrp9mvHiq66yjzVNTU1mfdasWWa9uro6thY6/2DlypVmffz48Wa91NcwSCJV2EXkMIAOABcA9KhqrhhNEVHxFePIPkNVjxfheYiohPg3O5ETacOuAP4hIrtEZGGhB4jIQhHJi0i+ra0t5e6IKKm0YZ+mqlMA3AtgsYj86vIHqOpqVc2paq6mpibl7ogoqVRhV9Xm6PMxAG8BmFqMpoio+BKHXUSGikj1pdsAfgNgX7EaI6LiSvNufC2At6LxwoEA1qvqu0XpypnQvO7QeHQara2tZr29vd2sf/vtt2Z9165difcdmis/cuRIsz5mzJjY2qlTp8xtc7krbxQ5cdhV9RCAfytiL0RUQhx6I3KCYSdygmEncoJhJ3KCYSdyglNcK0BoumVo6O3LL7+MrS1dutTc9uTJk2bdmiYKAI2NjWZ93LhxsbX9+/eb206fPt2sh6aZdnV1xdaqqqrMbUPDfllKenlwHtmJnGDYiZxg2ImcYNiJnGDYiZxg2ImcYNiJnOA4ewVIu3zvxIkTY2tr1641tx01alSqfZdS6MpG586dM+u33XZbbO3hhx82t7XODwDSnxthbR+6DPXAgcliyyM7kRMMO5ETDDuREww7kRMMO5ETDDuREww7kRMcZ7/ChcbRQ5exDo0npz1HwDJjxgyzvnnzZrM+YsSI2NqHH35obrt8+XKzXuqlsC3W5butefg8shM5wbATOcGwEznBsBM5wbATOcGwEznBsBM5wXH2K1xobnS05HastOPo1rhvaF72o48+atY3bdpk1q3v/eDBg+a2Z8+eNetDhgwx6yHWNfMXL15sbmtdL7+pqSm2Fjyyi8gaETkmIvv63DdSRN4XkS+iz/FnLxBRRejPr/FrAcy87L6nAdSr6k0A6qOviaiCBcOuqjsAtF929xwA66Lb6wDcX9y2iKjYkr5BV6uqLdHtowBq4x4oIgtFJC8i+ba2toS7I6K0Ur8br73vgsS+E6Kqq1U1p6q50AUEiah0koa9VUTGAkD0+VjxWiKiUkga9q0AFkS3FwDYUpx2iKhUguPsIrIBwHQAo0XkCIA/AHgJwN9F5DEAXwN4qJRNUnKhcfRSC10/3XLfffeZdWu+OmCvPT98+HBz2/r6erM+YcIEsz537lyzbjlx4oRZX79+fWytoaEhthYMu6rOjyn9OrQtEVUOni5L5ATDTuQEw07kBMNO5ATDTuQEp7heAaypnGmH3kJTZNNOoU0jNPzV0dERW2tvv3y6x/fNnj07UU+X1NbGnkEOwJ46HLqE9tixYxM9L4/sRE4w7EROMOxETjDsRE4w7EROMOxETjDsRE5wnP0KkOU01jRTWNP65JNPzPodd9wRW2tpaYmtAcDGjRvN+unTp836ihUrzHpnZ2ds7Z577jG3TYpHdiInGHYiJxh2IicYdiInGHYiJxh2IicYdiInOM7uXNr56NaSzAAwYMCAxM8d6q2qqsqsV1dXJ37utF588UWzfvHixdjagw8+WOx2APDITuQGw07kBMNO5ATDTuQEw07kBMNO5ATDTuQEx9kjoXFXa1w07ZhtaE54lnPGQ0K9pZlrn8vlzHro+urvvfde4n2HnD9/3qxfuHDBrN94442xtdGjRyfqKST4UyQia0TkmIjs63PfCyLSLCJ7oo9ZJemOiIqmP4eMtQBmFrj/T6o6KfrYVty2iKjYgmFX1R0A7LVyiKjipflj8AkR2Rv9mj8i7kEislBE8iKSb2trS7E7IkojadhXAZgIYBKAFgAr4x6oqqtVNaequZqamoS7I6K0EoVdVVtV9YKqXgTwFwBTi9sWERVborCLSN81Y+cC2Bf3WCKqDMFxdhHZAGA6gNEicgTAHwBMF5FJABTAYQCLitFMmrnVaedlh+rWvGzP0pwD8MADD5h167rvAPD6668n3rd13gQQ/r5C8/jPnDlj1idPnmzWSyEYdlWdX+Du10rQCxGVUOWemkVERcWwEznBsBM5wbATOcGwEzlRUVNc01xaOMtliz/77DOzvmbNGrP+1FNPmfU0Zx6mHWI6d+6cWb/66qvN+nPPPRdbC50+/eabb5r1NNJOGw5tH3rdJ06cmHjfSadU88hO5ATDTuQEw07kBMNO5ATDTuQEw07kBMNO5ERZx9lVFd3d3WbdYo1tDhxofyvWeC8AvPrqq2Z9zJgxZt3y1VdfmfUtW7aY9c8//zzxvkPjwaHXPDSO3tTUZNY3bdoUW9u2Ld11Ss+ePWvWhwwZEltLe/7BiRMnzHrovI9p06aZdQvH2YnIxLATOcGwEznBsBM5wbATOcGwEznBsBM5UdZxdhHBoEGDyrnL7+zevdust7a2mnVr3DQ0Znv99deb9WPHjpn1d955x6zPnj3brFvSXgdg/vxCFx/+fzNnFloTtFeaOd2APY5eakePHjXrQ4cONet33nlnMdvpFx7ZiZxg2ImcYNiJnGDYiZxg2ImcYNiJnGDYiZwo6zh7Z2cnduzYEVv/5ptvzO3nzZsXWwvNu25pabGbC7j22mtjayNGjDC3DY0Hh8ZklyxZYtbTjLOHzJkzx6w3Njaa9dBc/Z+qU6dOmfVrrrmmZPsu2Xx2EZkgIttFZL+INIrIkuj+kSLyvoh8EX22f+KJKFP9+TW+B8DvVfVWAP8OYLGI3ArgaQD1qnoTgProayKqUMGwq2qLqu6ObncA+BTAeABzAKyLHrYOwP0l6pGIiuBHvUEnInUAJgP4F4BaVb30h/BRALUx2ywUkbyI5E+ePJmiVSJKo99hF5FhADYDWKqqp/vWtPcdg4LvGqjqalXNqWruuuuuS9MrEaXQr7CLyCD0Bv1vqnppac1WERkb1ccCsKduEVGmgkNv0jsH8jUAn6rqH/uUtgJYAOCl6HNwjKWrqwuHDh2KrS9atMjc/vnnn4+tDRs2zNy2ubnZrIe2t6bmhi6nHNp32uV/ly1bFlt7/PHHzW2XL19u1rdv327W7777brM+atQos/5TFRrKra6uLtm+k05L7s84+y8B/A5Ag4jsie57Fr0h/7uIPAbgawAPJeqAiMoiGHZV/SeAuP9Kfl3cdoioVHi6LJETDDuREww7kRMMO5ETDDuRE5J0ulwSuVxO8/l8bD10ed39+/cn3ndoLDs0HtzT0xNba2trM7cNTXE9d+6cWQ/9G505c8asW2pqasx6aEz3gw8+MOu33357bC3tsslppN33qlWrzPobb7xh1uvr6816UrlcDvl8vuA/Go/sRE4w7EROMOxETjDsRE4w7EROMOxETjDsRE6U9VLSIXV1dWb9448/jq3dcMMN5rbnz58366Elm61x2dBc+K6uLrOedtlk61LWVVVVqZ57zJgxZt0aRw9J+32nEfo3CZ0bEbqUdG1twau09UvovIvQZdPj8MhO5ATDTuQEw07kBMNO5ATDTuQEw07kBMNO5ERFjbM/88wzZn3Dhg2xtdC120NzwkPX+R4+fHhsLTSWHRpPtubKA0B3d7dZt7630Lztjo4Os75+/XqzHmLtv5Tz1UPSXschNBaeZpw99G+WFI/sRE4w7EROMOxETjDsRE4w7EROMOxETjDsRE70Z332CQD+CqAWgAJYrap/FpEXAPwngEsXTX9WVbelaSY0N9oaG3333XfNbVesWGHWd+7cadZPnz5t1n+q7rrrLrM+Y8aMMnVSXmnH+D/66COzPm7cuMTPXap5/v05qaYHwO9VdbeIVAPYJSLvR7U/qer/lKQzIiqq/qzP3gKgJbrdISKfAhhf6saIqLh+1O8yIlIHYDKAf0V3PSEie0VkjYgUvDaSiCwUkbyI5EPLJBFR6fQ77CIyDMBmAEtV9TSAVQAmApiE3iP/ykLbqepqVc2pai60rhgRlU6/wi4ig9Ab9L+p6psAoKqtqnpBVS8C+AuAqaVrk4jSCoZdet8afA3Ap6r6xz73j+3zsLkA9hW/PSIqlv68G/9LAL8D0CAie6L7ngUwX0QmoXc47jCARSXor99mzpyZqh5y4MCB2NquXbvMbffu3WvWm5ubzXp7e7tZt4Zqxo+330t95ZVXzHpIaKpoltNYLWkvsb1s2TKzfvPNNyd+7sGDByfe1tKfd+P/CaDQT1OqMXUiKq/K/G+XiIqOYSdygmEncoJhJ3KCYSdygmEnckLSXlL3x8jlcprP58u2PyJvcrkc8vl8wRMveGQncoJhJ3KCYSdygmEncoJhJ3KCYSdygmEncqKs4+wi0gbg6z53jQZwvGwN/DiV2lul9gWwt6SK2duNqlrw+m9lDfsPdi6SV9VcZg0YKrW3Su0LYG9Jlas3/hpP5ATDTuRE1mFfnfH+LZXaW6X2BbC3pMrSW6Z/sxNR+WR9ZCeiMmHYiZzIJOwiMlNEPheRgyLydBY9xBGRwyLSICJ7RCTTyffRGnrHRGRfn/tGisj7IvJF9LngGnsZ9faCiDRHr90eEZmVUW8TRGS7iOwXkUYRWRLdn+lrZ/RVltet7H+zi8gAAAcA3APgCICdAOar6v6yNhJDRA4DyKlq5idgiMivAHQC+Kuq3hbd998A2lX1peg/yhGqurxCensBQGfWy3hHqxWN7bvMOID7AfwHMnztjL4eQhletyyO7FMBHFTVQ6p6HsBGAHMy6KPiqeoOAJcvBzMHwLro9jr0/rCUXUxvFUFVW1R1d3S7A8ClZcYzfe2Mvsoii7CPB9DU5+sjqKz13hXAP0Rkl4gszLqZAmpVtSW6fRRAbZbNFBBcxrucLltmvGJeuyTLn6fFN+h+aJqqTgFwL4DF0a+rFUl7/warpLHTfi3jXS4Flhn/TpavXdLlz9PKIuzNACb0+fpn0X0VQVWbo8/HALyFyluKuvXSCrrR52MZ9/OdSlrGu9Ay46iA1y7L5c+zCPtOADeJyM9FZDCA3wLYmkEfPyAiQ6M3TiAiQwH8BpW3FPVWAAui2wsAbMmwl++plGW845YZR8avXebLn6tq2T8AzELvO/JfAvivLHqI6esXAD6JPhqz7g3ABvT+WteN3vc2HgMwCkA9gC8AfABgZAX19r8AGgDsRW+wxmbU2zT0/oq+F8Ce6GNW1q+d0VdZXjeeLkvkBN+gI3KCYSdygmEncoJhJ3KCYSdygmEncoJhJ3Li/wC0GZI4MYLG+wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print an image\n",
    "print(train_lbls[9])\n",
    "image = train_imgs[9]\n",
    "plt.imshow(image, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_out (Dense)           (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "646/657 [============================>.] - ETA: 0s - loss: 0.5763 - accuracy: 0.7997INFO:tensorflow:Assets written to: .\\models\\20220402_121416\\assets\n",
      "657/657 [==============================] - 3s 4ms/step - loss: 0.5751 - accuracy: 0.7999 - val_loss: 0.5370 - val_accuracy: 0.8123 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "631/657 [===========================>..] - ETA: 0s - loss: 0.4139 - accuracy: 0.8501INFO:tensorflow:Assets written to: .\\models\\20220402_121416\\assets\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.4133 - accuracy: 0.8505 - val_loss: 0.3963 - val_accuracy: 0.8579 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.3671 - accuracy: 0.8677 - val_loss: 0.4527 - val_accuracy: 0.8368 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "626/657 [===========================>..] - ETA: 0s - loss: 0.3388 - accuracy: 0.8764\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.3374 - accuracy: 0.8768 - val_loss: 0.4138 - val_accuracy: 0.8561 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "649/657 [============================>.] - ETA: 0s - loss: 0.2827 - accuracy: 0.8972INFO:tensorflow:Assets written to: .\\models\\20220402_121416\\assets\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.2824 - accuracy: 0.8973 - val_loss: 0.3262 - val_accuracy: 0.8851 - lr: 1.0000e-04\n",
      "Epoch 6/10\n",
      "636/657 [============================>.] - ETA: 0s - loss: 0.2730 - accuracy: 0.9014INFO:tensorflow:Assets written to: .\\models\\20220402_121416\\assets\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.2741 - accuracy: 0.9008 - val_loss: 0.3238 - val_accuracy: 0.8857 - lr: 1.0000e-04\n",
      "Epoch 7/10\n",
      "657/657 [==============================] - ETA: 0s - loss: 0.2706 - accuracy: 0.9014INFO:tensorflow:Assets written to: .\\models\\20220402_121416\\assets\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.2706 - accuracy: 0.9014 - val_loss: 0.3216 - val_accuracy: 0.8850 - lr: 1.0000e-04\n",
      "Epoch 8/10\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.2678 - accuracy: 0.9026 - val_loss: 0.3220 - val_accuracy: 0.8844 - lr: 1.0000e-04\n",
      "Epoch 9/10\n",
      "633/657 [===========================>..] - ETA: 0s - loss: 0.2651 - accuracy: 0.9037INFO:tensorflow:Assets written to: .\\models\\20220402_121416\\assets\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.2650 - accuracy: 0.9038 - val_loss: 0.3186 - val_accuracy: 0.8863 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.2625 - accuracy: 0.9045 - val_loss: 0.3188 - val_accuracy: 0.8864 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25e4224ed70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the keras sequential model\n",
    "model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128, activation='relu', name='dense_1'),\n",
    "    layers.Dense(10, activation='softmax', name='dense_out')\n",
    "])\n",
    "\n",
    "# print the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# compile the model\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# get the required directories\n",
    "check_dir, tboard_dir = make_directories()\n",
    "\n",
    "# using the callbacks\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=check_dir,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir=tboard_dir\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# fit the model with training data with a validation split\n",
    "# by default .fit will shuffle the data\n",
    "model.fit(\n",
    "    train_imgs, \n",
    "    train_lbls, \n",
    "    validation_split=0.3, \n",
    "    epochs=10, \n",
    "    batch_size=64, \n",
    "    callbacks=[reduce_lr, early_stop, checkpoints, tensorboard]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3483 - accuracy: 0.8781\n",
      "[0.34828975796699524, 0.8780999779701233]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83      1000\n",
      "           1       0.99      0.96      0.98      1000\n",
      "           2       0.77      0.83      0.80      1000\n",
      "           3       0.85      0.91      0.88      1000\n",
      "           4       0.79      0.79      0.79      1000\n",
      "           5       0.97      0.95      0.96      1000\n",
      "           6       0.73      0.64      0.68      1000\n",
      "           7       0.92      0.95      0.94      1000\n",
      "           8       0.96      0.97      0.97      1000\n",
      "           9       0.96      0.95      0.96      1000\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test data\n",
    "test_evals = model.evaluate(test_imgs, test_lbls)\n",
    "print(test_evals)\n",
    "# predict on the test data\n",
    "predictions = model.predict(test_imgs)\n",
    "predicted_lbls = np.argmax(predictions, axis=1)\n",
    "cr = classification_report(test_lbls, predicted_lbls)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28)]          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_out (Dense)           (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "642/657 [============================>.] - ETA: 0s - loss: 0.5659 - accuracy: 0.8017INFO:tensorflow:Assets written to: .\\models\\20220402_121434\\assets\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.5635 - accuracy: 0.8024 - val_loss: 0.5142 - val_accuracy: 0.8212 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "635/657 [===========================>..] - ETA: 0s - loss: 0.4063 - accuracy: 0.8545INFO:tensorflow:Assets written to: .\\models\\20220402_121434\\assets\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.4060 - accuracy: 0.8547 - val_loss: 0.4054 - val_accuracy: 0.8533 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.3618 - accuracy: 0.8693 - val_loss: 0.4325 - val_accuracy: 0.8425 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "641/657 [============================>.] - ETA: 0s - loss: 0.3341 - accuracy: 0.8778\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.3337 - accuracy: 0.8779 - val_loss: 0.4224 - val_accuracy: 0.8530 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "627/657 [===========================>..] - ETA: 0s - loss: 0.2798 - accuracy: 0.8985INFO:tensorflow:Assets written to: .\\models\\20220402_121434\\assets\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.2788 - accuracy: 0.8988 - val_loss: 0.3233 - val_accuracy: 0.8846 - lr: 1.0000e-04\n",
      "Epoch 6/10\n",
      "646/657 [============================>.] - ETA: 0s - loss: 0.2699 - accuracy: 0.9019INFO:tensorflow:Assets written to: .\\models\\20220402_121434\\assets\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.2705 - accuracy: 0.9014 - val_loss: 0.3203 - val_accuracy: 0.8853 - lr: 1.0000e-04\n",
      "Epoch 7/10\n",
      "627/657 [===========================>..] - ETA: 0s - loss: 0.2672 - accuracy: 0.9028INFO:tensorflow:Assets written to: .\\models\\20220402_121434\\assets\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.2671 - accuracy: 0.9025 - val_loss: 0.3178 - val_accuracy: 0.8869 - lr: 1.0000e-04\n",
      "Epoch 8/10\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.2643 - accuracy: 0.9035 - val_loss: 0.3187 - val_accuracy: 0.8853 - lr: 1.0000e-04\n",
      "Epoch 9/10\n",
      "653/657 [============================>.] - ETA: 0s - loss: 0.2612 - accuracy: 0.9047INFO:tensorflow:Assets written to: .\\models\\20220402_121434\\assets\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.2612 - accuracy: 0.9049 - val_loss: 0.3162 - val_accuracy: 0.8872 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "631/657 [===========================>..] - ETA: 0s - loss: 0.2582 - accuracy: 0.9057INFO:tensorflow:Assets written to: .\\models\\20220402_121434\\assets\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.2588 - accuracy: 0.9054 - val_loss: 0.3156 - val_accuracy: 0.8868 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25e429b8430>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# limitations with the sequential model\n",
    "# single input and output\n",
    "# sequence of layers\n",
    "\n",
    "# using the functional API\n",
    "# can be used with multiple inputs and outputs\n",
    "\n",
    "# using the keras functional API\n",
    "inputs = keras.Input(shape=(28,28))\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(128, activation='relu', name='dense_1')(x)\n",
    "\n",
    "outputs = layers.Dense(10, activation='softmax', name='dense_out')(x)\n",
    "\n",
    "# creating the Model by grouping the layers\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# print the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# compile the model\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# get the required directories\n",
    "check_dir, tboard_dir = make_directories()\n",
    "\n",
    "# using the callbacks\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=check_dir,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir=tboard_dir\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# fit the model with training data with a validation split\n",
    "# by default .fit will shuffle the data\n",
    "model.fit(\n",
    "    train_imgs, \n",
    "    train_lbls, \n",
    "    validation_split=0.3, \n",
    "    epochs=10, \n",
    "    batch_size=64, \n",
    "    callbacks=[reduce_lr, early_stop, checkpoints, tensorboard]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 869us/step - loss: 0.3456 - accuracy: 0.8793\n",
      "[0.345630407333374, 0.8792999982833862]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.84      1000\n",
      "           1       0.99      0.96      0.98      1000\n",
      "           2       0.76      0.82      0.79      1000\n",
      "           3       0.86      0.91      0.88      1000\n",
      "           4       0.79      0.79      0.79      1000\n",
      "           5       0.96      0.96      0.96      1000\n",
      "           6       0.73      0.64      0.68      1000\n",
      "           7       0.92      0.96      0.94      1000\n",
      "           8       0.96      0.97      0.97      1000\n",
      "           9       0.97      0.94      0.96      1000\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test data\n",
    "test_evals = model.evaluate(test_imgs, test_lbls)\n",
    "print(test_evals)\n",
    "# predict on the test data\n",
    "predictions = model.predict(test_imgs)\n",
    "predicted_lbls = np.argmax(predictions, axis=1)\n",
    "cr = classification_report(test_lbls, predicted_lbls)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.fashion_mnist\n",
    "(train_imgs, train_lbls), (test_imgs, test_lbls) = data.load_data()\n",
    "\n",
    "# reshape and normalize data for CNN\n",
    "train_imgs = train_imgs.reshape((60000, 28, 28, 1)) \n",
    "train_imgs = train_imgs.astype(\"float32\") / 255 \n",
    "test_imgs = test_imgs.reshape((10000, 28, 28, 1)) \n",
    "test_imgs = test_imgs.astype(\"float32\") / 255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 0 0 ... 3 0 5]\n"
     ]
    }
   ],
   "source": [
    "print(train_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(28, 28, 1)) \n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs) \n",
    "x = layers.MaxPooling2D(pool_size=2)(x) \n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x) \n",
    "x = layers.MaxPooling2D(pool_size=2)(x) \n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x) \n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x) \n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# compile the model\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# get the required directories\n",
    "check_dir, tboard_dir = make_directories()\n",
    "\n",
    "# using the callbacks\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=check_dir,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir=tboard_dir\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# fit the model with training data with a validation split\n",
    "# by default .fit will shuffle the data\n",
    "model.fit(\n",
    "    train_imgs, \n",
    "    train_lbls, \n",
    "    validation_split=0.3, \n",
    "    epochs=10, \n",
    "    batch_size=64, \n",
    "    callbacks=[reduce_lr, early_stop, checkpoints, tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test data\n",
    "test_evals = model.evaluate(test_imgs, test_lbls)\n",
    "print(test_evals)\n",
    "# predict on the test data\n",
    "predictions = model.predict(test_imgs)\n",
    "predicted_lbls = np.argmax(predictions, axis=1)\n",
    "cr = classification_report(test_lbls, predicted_lbls)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use \"image_dataset_from_directory\" to read, shuffle, resize, batch, etc.. to load images\n",
    "* In case the model overfits, use different data augmentation strategies such as: RandomFlip, RandomRotation, RandomZoom\n",
    "* In case your dataset is small, use a pretrained model (i.e., if the model is well generalized)\n",
    "    * e.g., ImageNet, ResNet"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcacb1169d0317e97b762e0c1a317633c556cb59cb1017287d7e24b96b766e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
